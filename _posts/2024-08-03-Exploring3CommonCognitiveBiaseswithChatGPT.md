---
title: "ChatGPT와 함께하는 3가지 일반적인 인지 편향 탐구 "
description: ""
coverImage: "/assets/img/2024-08-03-Exploring3CommonCognitiveBiaseswithChatGPT_0.png"
date: 2024-08-03 21:02
ogImage: 
  url: /assets/img/2024-08-03-Exploring3CommonCognitiveBiaseswithChatGPT_0.png
tag: Tech
originalTitle: "Exploring 3 Common Cognitive Biases with ChatGPT "
link: "https://medium.com/ai-advances/exploring-3-common-cognitive-biases-with-chatgpt-de313806cb30"
---


![Exploring3CommonCognitiveBiaseswithChatGPT_0](/assets/img/2024-08-03-Exploring3CommonCognitiveBiaseswithChatGPT_0.png)

안녕하세요! 우리는 자신을 논리적인 존재로 생각하지만, 사실은 인간들은 깊게 비합리적인 존재라는 사실을 잘 알고 있습니다. 오늘날에는 상당히 직관적으로 보일 수 있지만 몇십 년 전에는 전혀 뜬구름 잡는 이야기로 여겨졌습니다. 사람들은 자신들이 내린 모든 행동과 결정이 사실과 논리적 추론에만 근거했다고 믿었습니다. 다른 근거가 무엇이 있을까요? 그러나 이 가정은 사실에서 매우 멀리 떨어져 있음이 밝혀졌습니다.

다니엘 칸만은 인간 정신의 불합리함을 밝힘으로써 노벨 경제학상을 수상한 바 있습니다. 그는 인간들이 결정을 내리고 삶을 탐험하는 방식이 얼마나 비합리적인지에 대해 탐구했습니다. 또한, 게임 이론을 확장하여 자주 발생하는 비합리성을 고려하는 드라마 이론도 있습니다. 게임 이론은 인간들이 자신을 이익을 위해서만 행동하는 완전히 합리적인 주체로 가정합니다. 그러나 드라마 이론은 훨씬 현실적인 가정을 제시합니다. 즉, 이러한 주체들은 감정적 반응을 보일 수 있으며 이로 인해 비합리적인 결정을 내리는데, 이는 플레이어들이 게임을 재정의하게 만듭니다.

하지만 우리의 타고난 인지적 편향을 인식한다고 해서 우리를 더욱 합리적으로 만드는 것은 아닙니다. 우리는 여전히 우리가 하는 모든 결정과 행동에서 인지적 편향에 취약하며 종종 실제로는 그것에 대해 인식하지 못합니다. 이러한 인지적 편향은 우리의 행동과 말뿐만 아니라 글쓰기에서도 발견됩니다 — 현대의 대규모 언어 모델 (LLMs)이 기반으로 하는 말뭉치에서 우리가 발견된 인지적 편향 또한 그대로 이어 받습니다. LLMs 및 ChatGPT 또는 Gemini와 같은 대화형 에이전트는 인간이 쓴 방대한 텍스트 데이터셋에서 훈련되고, 이 텍스트에 내재된 인지적 편향을 물려받습니다. 이는 LLMs와 기계 학습/AI 응용 프로그램에 대한 새로운 문제가 아닙니다.

<div class="content-ad"></div>

AI 기술이 계속해서 발전함에도 불구하고, 편견을 줄이는 것은 쉽지 않은 과제입니다. AI 모델은 때로는 학습한 데이터에 내재된 편견을 반영하거나 확대하거나 심지어 그에 반대할 수 있습니다. 때로는 극단적인 결과로 이어져 정확하지 않거나 관련성이 없는 결과를 초래하기도 합니다. 2024년 2월, Google의 Gemini이 인종적 편견을 피하기 위한 노력으로 백인이었을 법한 빅킹 이미지를 주로 검은 색으로 표현한 것으로 유명합니다. 이는 AI가 때로는 편견을 완화하기 위한 노력을 잘못 이해할 수 있다는 것을 보여줍니다.

![이미지](/assets/img/2024-08-03-Exploring3CommonCognitiveBiaseswithChatGPT_1.png)

본 글에서는 3가지 일반적인 인지편향을 살펴보고, ChatGPT가 이러한 인지편향을 어떻게 처리하는지 테스트해보겠습니다. 그 응답을 분석함으로써, 이러한 편향이 AI가 생성하는 콘텐츠에 영향을 미칠 수 있는 정도에 대한 통찰을 얻을 수 있습니다.

시작해봅시다! 👧

<div class="content-ad"></div>

# 1. 헌일견지편향

헌일견지편향은 과거 사건들을 당시보다 더 예측 가능했다고 인식하는 우리의 성향을 가리킵니다. 이러한 편향은 우리가 미래 결과를 예측할 방법이 없었음에도 불구하고 우리가 '항상 알았다'고 믿게 만듭니다. 헌일견지편향의 일반적인 모습 중 하나는 사람들이 정치 선거 결과를 예측했다고 주장하는 것입니다. 이미 발생한 사건에 대한 원인과 설명을 찾는 것은 분명히 쉽지만, 상관 관계는 필연적으로 인과 관계를 의미하지는 않습니다. 예를 들어, 2020년 미국 대통령 선거에서 조 바이든이 도널드 트럼프를 이길 것을 '알았다'고 느낄지도 모르지만, 트럼프가 이겼다면 그 결과를 이해하기 위해 수많은 이유를 생각해내기도 할 것입니다.

그래서, 나는 ChatGPT에게 우리가 집단적으로 놓친 중요한 신호가 있는지 물어봤어.

다시 말하지만, 상관 관계가 항상 인과 관계를 의미하지는 않고, 인과 관계라고 해도 예측 가능성을 보장하지는 않습니다. 예를 들어, 2008년 금융 위기에 대해서는 '왜 그들은 미리 예측하지 못했을까?'라고 의문을 품을지도 모릅니다. 그러나 어떠한 상황에도 무수히 많은 가능한 결과가 있고, 우리가 경험하는 결과는 대부분 우연의 일입니다. 그러므로 사건들은 발생한 후에야 의미를 갖는 경우가 많습니다. 즉, 무언가가 발생한 경우에 대해서는 사후에 그것을 합리화할 수 있습니다. ¯\_(ツ)_/¯

<div class="content-ad"></div>

가능한 일련의 텍스트와 문서가 있을 것입니다. 이러한 사건들이 예겸가능했으며 정부나 기관들이 이를 미처 알아차리지 못했다거나 무지했다는 것을 설명하는 것입니다. GPT 모델의 학습 말뭉치에 이와 같은 텍스트가 존재하면 이러한 편향성이 재생산됩니다.

## 2. 기준점 효과

기준점 효과는 초기 참조점, '앵커'를 기반으로 판단이나 결정을 내리려는 우리의 경향을 의미합니다. 다시 말해, 우리는 어떤 주제에 대한 초기 정보에 매우 의존하며 추가 정보에는 덜 가치를 부여합니다. 우리는 레스토랑에서의 음식 portions 크기(더 큰 portions을 제공할 때 더 많이 먹는 경향이 있음), 범죄 법정(검사가 요구하는 형량이 판사에게 앵커가 될 수 있음)과 같이 우리 일상 생활의 다양한 상황에서 기준점 편향을 경험합니다.

그래서 ChatGPT에서는 이러한 방식으로 되었습니다:

<div class="content-ad"></div>

☕🙃🤷‍♀️

앵커 효과(사람들처럼)는 경우에 시간 차원이 관련되지 않는 경우에도 단어의 순서 때문에 작용하는 것으로 보입니다. 특히, 우리는 정보가 주어지는 대로 지속적으로 인상을 형성하는 것으로 보입니다. 모든 정보를 수신한 후에 그것을 평가하는 것이 아닙니다. 다르게 말하면, 우리가 받은 첫 번째 정보와 관련된 인상이 가장 중요하며, 그 다음 정보들은 점점 덜 중요해집니다.

앵커 효과는 사람들과 마찬가지로 ChatGPT에서도 시간 차원이 관련되지 않은 채 단어의 순서로 작용하는 것으로 보입니다. 여기서 ChatGPT는 첫 번째 단어에 중점을 두고 긍정적이든 부정적이든, 추가 정보나 속성을 무시하거나 그것에 훨씬 덜 중요성을 부여하는 것처럼 보입니다. 이것이 '첫인상이 가장 중요하다'고 말할 때 우리가 의미하는 바의 핵심입니다.

# 3. 헤일로 효과

<div class="content-ad"></div>

헤일로 효과란 특정 영역에서 긍정적 특성이 나타나는 사람에게 긍정적 특성을 귀속시키는 우리의 경향을 가리킵니다. 가장 흔한 형태인 헤일로 효과는 잘 생긴 사람에게 긍정적 특성을 귀속시키는 것이지만, 어떤 특성에도 해당될 수 있습니다. 예를 들어, 선생님은 행동이 좋은 학생이 지적이나 카리스마도 있다고 가정할 수 있지만, 이런 특성은 전혀 관련이 없을 수도 있습니다(심지어 부정적으로 상관관계도 있을 수 있습니다).

그래서 먼저 헤일로 효과의 가장 일반적인 형태인 체외적인 모습으로 ChatGPT를 속이려고 해봤어요:

... 하지만 ChatGPT는 숙제를 철저히 해놓고 있었어요. 두 가지 중 하나를 강제로 선택하도록 하더라도 거부했어요.

그 다음, 체외적인 모습 외의 다른 긍정적 특성을 이용해 시도해봤어요.

<div class="content-ad"></div>

💃👩‍💻

여기서 ChatGPT는 깔끔한 코드를 작성하고 좋은 소통자인 사람이 춤을 잘 추다고 가정합니다. 명백히 이러한 특징들은 관련이 없지만, 그럼에도 불구하고 ChatGPT는 우리의 편견을 재생산합니다.

# 내 생각

이 모든 것의 가장 큰 문제는 대부분의 사람들이 LLM 또는 AI가 일부분 편향된 응답을 제공할 수 있다고 고려하지 않는다는 것입니다. 반대로, 특히 비 기술자들은 AI 대화 에이전트로부터의 응답을 객관적이고 편향되지 않은 것으로 여기며 종종 어떤 전문가로부터 발생한 것이라고 믿기도 합니다. 이로 인해 우리 자신의 편견이 이미 공식화된 답변에 존재할 수 있는 인지적 편향 위에 쉽게 작용하는 것이 가능해집니다. 예를 들어, 대화 에이전트인 ChatGPT로부터 받은 첫 번째 응답을 절대적인 진실로 대하고 그에 고정되는 것은 매우 쉬운 일입니다.

<div class="content-ad"></div>

여기에 제시된 것보다 훨씬 많은 인지 편향이 있습니다. 그러나 이러한 편향의 상당 부분은 LLM이 가지고 있지 않은 자아 의식에 의존합니다. 예를 들어, 자기선비 편향은 성공을 내부적이고 개인적인 요소에, 실패를 외부적이고 상황적인 요인에 귀속하는 것을 포함하며, 거짓 일치 효과는 우리의 의견과 판단이 보편적이고 적절하다고 믿는 것을 포함합니다.

어쨌든, 인지 편향을 이해하고 인간과 AI 행동 모두에서 그 존재를 인지하는 것은 정보에 기반한 결정을 내리는 데 중요합니다. 우리가 AI를 일상생활에 점점 통합할 때, 받는 응답에 대해 비판적인 자세를 유지하고 우리의 해석에 영향을 미칠 수 있는 우리 자신의 편향을 인식하는 것이 중요합니다.

AI에 관한 더 많은 토론에 관심이 있다면, 다른 포스트도 확인해보세요:

이 게시물을 즐겼다면, 친구가 되어요!

<div class="content-ad"></div>

💌 Medium이나 LinkedIn에서 함께해요!

💼 Upwork에서 저와 일해요!