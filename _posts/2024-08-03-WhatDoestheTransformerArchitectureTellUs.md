---
title: "트랜스포머 아키텍처가 알려주는 것들"
description: ""
coverImage: "/assets/img/2024-08-03-WhatDoestheTransformerArchitectureTellUs_0.png"
date: 2024-08-03 21:10
ogImage: 
  url: /assets/img/2024-08-03-WhatDoestheTransformerArchitectureTellUs_0.png
tag: Tech
originalTitle: "What Does the Transformer Architecture Tell Us"
link: "https://medium.com/towards-data-science/what-does-the-transformer-architecture-tell-us-cd3a4fd6a59d"
---


![이미지](/assets/img/2024-08-03-WhatDoestheTransformerArchitectureTellUs_0.png)

대규모 언어 모델(Large Language Models; LLMs)인 ChatGPT와 같은 뛰어난 성능은 세계를 놀라게 했습니다. 이 혁신은 단순하면서도 확장 가능한 Transformer 아키텍처의 발명으로 이루어졌습니다. 여전히 심층 학습 신경망으로 구성되어 있지만 주요 추가 요소는 각 단어 토큰을 맥락화하는 "주의(attention)" 메커니즘입니다. 또한 이례적인 병렬성으로 LLMs에 대규모 확장성과 따라서 수십억 개의 매개변수로 훈련한 후 놀라운 정확도를 제공합니다.

Transformer 아키텍처가 보여준 단순성은 실제로 튜링 기계(Turing machine)와 비교될 수 있습니다. 차이점은 튜링 기계가 각 단계에서 기계가 무엇을 할 수 있는지를 제어한다는 것입니다. 그러나 Transformer는 대량의 입력 데이터로부터 매개변수 최적화를 통해 학습하는 마법 상자처럼 작동합니다. 연구원들과 과학자들은 여전히 Transformer의 잠재력과 인간의 마음을 연구하는 데 어떤 이론적 함의가 있는지에 깊은 관심을 가지고 있습니다.

본문에서는 먼저 Transformer 아키텍처의 네 가지 주요 기능인 단어 임베딩, 주의 메커니즘, 단어 예측, 그리고 다중 모달 확장 및 전이 학습과 같은 일반화 기능에 대해 논의할 것입니다. 읽자분들이 많은 우수한 기사를 Medium에서 찾을 수 있는 Transformer 아키텍처를 구축하는 방법에 중점을 두는 대신, 아키텍처가 왜 이렇게 효과적인지에 초점을 맞출 것입니다. 그런 다음 AI의 미래를 위한 신흥 운영 체제뿐만 아니라 우리 뇌가 어떻게 작동하는지 설명하는 데 도움이 될 수 있는지 탐구할 것입니다. 마지막으로, 오늘날 또는 미래에 LLM이 의식을 가질 수 있는지에 대해 논의할 것입니다.

<div class="content-ad"></div>

# 트랜스포머 아키텍처의 주요 기능

## 워드 임베딩

워드 임베딩은 새로운 기술이 아닙니다. 이것은 1990년대에 각광받기 시작한 자연어 처리(NLP)의 기초였습니다. 워드 임베딩은 고차원 공간에서 벡터를 사용하여 단어(또는 부분 단어)를 표현합니다. 각 차원은 특성, 범주 또는 개념(예: 색상, 성별, 왕족, 음식 등)일 수 있습니다. 벡터와 차원의 교차점은 훈련을 통해 경사 하강 과정을 통해 조정할 수 있는 매개변수를 구성합니다. 아래는 워드 임베딩의 기본 개념을 보여주는 간단한 예제입니다.

![워드 임베딩 개념](/assets/img/2024-08-03-WhatDoestheTransformerArchitectureTellUs_1.png)

<div class="content-ad"></div>

동일한 특성을 가진 단어들은 동일한 차원을 따라서 정리되며, 공간에서 서로 가까운 단어들은 유사하거나 관련성이 있을 것으로 기대됩니다. 따라서, 단어 벡터는 벡터 덧셈, 뺄셈 또는 곱셈(즉, 곱셈)과 같은 선형 대수 연산을 통해 계산되고 추론될 수 있습니다. 결과적으로, 벡터 간의 거리는 단어들 간의 유사도의 정도를 인코딩하며, 벡터의 위치와 방향은 해당 단어들 간의 관계를 반영합니다.

우리는 높은 차원의 공간(3 이상)을 머리 속에서 상상하는 것은 불가능합니다. 그것을 이해하기 위한 전형적인 접근 방법은 이러한 고차원 벡터를 2차원 공간으로 투영하는 것입니다. 아래의 차트는 단순히 설명을 위한 예제입니다. 비슷한 단어인 Kings와 Queens가 서로 가깝고, 사과, 오렌지 및 포도는 다른 클러스터를 형성하는 것을 보여줍니다.

예를 들어, Transformer 아키텍처를 소개한 초기 논문에서 각 단어 임베딩은 입력 레이어에서 512개의 특성을 갖고 내부 레이어에서 2048개의 특성을 가졌습니다. 단어 토큰의 개수는 다양한 학습 세트에서 25,000에서 41,000까지 다양했습니다. 512에서 2024 차원의 행렬은 방대한 양의 학습 텍스트로부터 배운 단어의 의미를 인코딩했지만, 각 차원이 나타내는 정확한 특징은 알려져 있지 않습니다.

<div class="content-ad"></div>

특성 차원은 모델 스스로 학습하는 구문 규칙을 포함할 수도 있어요. 다시 말해, LLMs는 문법을 암묵적으로 학습할 수 있어요. 전형적인 예로는 프로그램이 셰익스피어의 모든 텍스트를 학습하게 한 후, 모델의 출력이 꽤 셰익스피어 언어와 비슷하게 말할 수 있어요. 내부적으로 각 단어의 예측은 가장 가능성 있는 단어를 찾기 위한 벡터 계산으로 이루어져요.

## 주의 메커니즘

Transformer의 주의 메커니즘은 단어와 문맥간의 유사성과 중요성을 계산하는 것을 포함해요. 기술적으로, 이것은 단어 벡터의 내적을 계산한 후 softmax 함수에 의해 정규화하는 것을 말해요. 문맥은 같은 문장 내 이전 단어 또는 전체 단어 임베딩에서 어떤 단어든 될 수 있어요. 전자는 "자기 주의(self-attention)"라고 부르고 후자는 "교차 주의(cross-attention)"라고 부르는데요.

내적은 기하학적 정의를 사용하여 더 직관적으로 이해될 수 있어요. 두 벡터 A와 B가 있다고 가정해보죠. 내적은 그들의 유클리드 크기의 곱과 그들 사이 각도의 코사인인데요, 아래 차트에 나와 있는 것처럼요(방정식 1). 기하학적으로 보면, 두 벡터 사이의 각이 작을수록 코사인 값과 내적이 커지며, 두 단어는 특정 맥락에서 관련이 더 크다는 것을 나타내요. 각 단어 벡터는 수백 개 또는 수천 개의 특성 차원을 갖고 있기 때문에, 내적은 어떤 차원에서는 튼튼하고 다른 차원에서는 약할 수 있어요.

<div class="content-ad"></div>


![Transformer Architecture](/assets/img/2024-08-03-WhatDoestheTransformerArchitectureTellUs_3.png)

Interestingly, A와 B의 상관 계수가 또한 각도의 코사인인 것이 흥미롭습니다 (위의 (2) 및 (3)의 식을 참조하세요). 각도가 작을수록 A와 B는 더 연관성이 있으며 특정 기능 또는 개념을 공유합니다. 각도가 90도일 때, A와 B의 상관 관계 및 점곱도 0이며, 이것은 둘이 관련이 없으며 서로 대각선상에 있음을 의미합니다. 각도가 180도일 때, 그들은 음의 상관 관계에 있고, 코사인은 -1입니다.

게다가, Transformer 어텐션 매트릭스는 일반적인 데이터베이스 검색 프로세스를 모방하기 위해 쿼리(Q), 키(K), 값(V)의 세 가지 일반적인 구조를 따릅니다. 쿼리는 요청된 정보이고; 키는 키워드 또는 특징이며; 값은 키와 연결된 내용입니다. 쿼리와 키의 점곱은 주의 점수를 생성하며, 그것들은 각 단어의 가중 합을 계산하기 위해 적용됩니다.

일반적인 비유는 YouTube에서 동영상을 검색하는 것입니다. 검색 창에 입력된 텍스트가 쿼리입니다. 검색 엔진은 먼저 쿼리의 단어를 일련의 키(예: 동영상 제목, 키워드, 태그 등)와 비교하여 관련성 점수를 계산합니다. 이 점수는 각 키에 연결된 후보 동영상에 적용됩니다. 마지막으로, 가장 일치하는 동영상과 가장 높은 합계 점수를 가진 동영상이 시청자에게 제공됩니다.


<div class="content-ad"></div>

100K개 이상의 단어 벡터와 512에서 2048개의 차원을 가진 임베딩을 감안할 때, 모든 가능한 단어 쌍 간의 내적을 계산하는 것은 계산적으로 매우 부담스럽습니다. 따라서, 멀티 헤드 어텐션 메커니즘은 특성 차원별로 작업을 병렬로 분할합니다. 예를 들어, 512개의 입력 차원이 있고 각 헤드 분할당 4개의 차원이 있는 경우, 멀티 헤드 어텐션은 512/4 = 128개의 병렬 프로세스로 이루어집니다. 출력 및 내부 네트워크 레이어에도 유사한 병렬 프로세스가 필요합니다. 이 대규모 병렬 처리는 충분한 GPU를 할당하여 작업을 완료하는 데 중요한 역할을 합니다.

## 한 번에 한 단어 예측하기

마지막으로, LLM 모델은 각 단어를 반복하고 다음으로 가장 가능성 높은 단어를 찾는 시퀀스 예측 프로그램입니다. 예측이 가능해지는 이유는 설정된 임베딩이 모든 단어를 인코딩하고 그들의 관계, 위치 및 문맥을 포함하기 때문입니다.

어텐션 메커니즘을 통해 현재 질문 또는 문장은 질문 행렬이 되며, 키와 값을 상호 작용하여 유사한 문맥을 갖는 가장 관련성 높은 단어를 검색합니다.

<div class="content-ad"></div>

예측은 Transformer의 출력 끝에서 계산된 확률에 기반합니다. 한 번 단어가 선택되면 그것을 사용하여 다음 단어를 예측하고, 그렇게 계속됩니다. 이것은 역설적인 것처럼 들립니다: 모든 ChatGPT의 의미 있는 텍스트와 요약은 확률과 변동성에 의해 단어별로 단순히 생성됩니다. 이는 프롬프트나 쿼리가 입력되기 전에 모든 중요한 작업이 수행되었고 단어의 확률이 광범위한 학습에서 인코딩되었기 때문입니다. 단어 예측은 단어 임베딩의 기초 위에 작용하며 타깃 컨텍스트 검색(즉, 셀프 어텐션)에 의해 추진됩니다.

또다른 이유는 이러한 단순한 단어별 예측이 어려운 이유는 전통적인 NLP가 언어에 대해 사람들이 생각하는 방식에 의해 주도되었는데, 여기서는 문법과 구문 규칙이 가장 먼저 옵니다. Transformer는 그에 의존하지 않습니다. 그것은 대규모 텍스트 말뭉치에서 배우고 최종 확률로서 그것을 부호화합니다.

게다가, AI 연구자들은 지난 몇 10년 동안 기계가 구조가 덜하고 복잡한 알고리즘을 통해 더 잘 배운다는 것을 배웠습니다. 대규모 컴퓨터 성능에 의해 지원되는 방대한 수의 매개변수는 AI 모델을 성공시키는 데 중요한 역할을 했습니다. OpenAI 연구원 Hyung Won Chung이 강의 중에 말한 대로 "지난 70년 동안 AI의 발전에서 학습한 깊은 교훈은 다음과 같다: 보다 일반적인 방법을 보다 약한 모델 가정과 함께 개발하고, 더 많은 데이터 및 계산(즉, 스케일 업)을 추가하라"는 것입니다. 구조와 모델링 가정이 적을수록 모델은 더 확장 가능해지는 것이다. 이것이 Transformer에 일어난 일입니다.

<div class="content-ad"></div>

트랜스포머 구조는 텍스트뿐만 아니라 다른 입력 형식도 처리할 수 있습니다. 첫 번째 적용 사례는 시각 정보였습니다. 2020년 말에 구글 연구원들은 컨볼루션 신경망(CNN)과 완전히 독립적인 첫 번째 트랜스포머 기반 시각 모델을 보고했습니다. 이 모델은 Vision Transformer(ViT)라고 불립니다. 이 모델은 이미지를 16x16 패치로 분할하고, 각 패치를 벡터로 토큰화하여 이미지 끝에 분류 텍스트를 패드하고, 단어 표현의 시퀀스로 트랜스포머 인코더에 벡터를 공급합니다.

출력에서, 분류 문제는 단어 예측 과제로 변환됩니다. 이미지가 제시되면 모델은 이미지를 인식하고 시퀀스에서 다음 분류 단어를 예측하여 이미지를 인식합니다. 이는 시각 처리에 그치지 않고 이미지와 텍스트를 교차하는 이중적인 처리입니다.

위에서 설명한 대로, 벡터 점 곱을 사용하여 자기 주의 메커니즘이 단어 시퀀스에서 중요한 단어를 강조합니다. 비슷하게, 이 함수는 특정 부분이 중요하지 않은 이미지 부분을 '취소'함으로써 이미지에서 관련된 객체나 기능을 강조합니다. 이 기능은 CNN의 컨볼루션 필터와 유사합니다.

ViT의 성공을 고려할 때, 텍스트, 이미지, 비디오를 포함한 다중 입력 형식을 가진 트랜스포머 모델들이 번창하며 오늘날도 빠르게 발전하고 있습니다. 다중 입력 형식 능력은 트랜스포머에 거의 무한한 학습 잠재력을 부여하며, 이미지, 비디오, 오디오 등 특정 출력 형식을 가지고 특정 작업을 수행할 수 있으며, 게임을 하거나 로봇 동작을 제어할 수도 있습니다.

<div class="content-ad"></div>

또 다른 개념은 문맥에 맞게 조절된 세포주의를 가진 훈련된 임베딩이 다른 트랜스포머 모델로 전이 가능하다는 것입니다. 이 "전이 학습"은 매우 유익합니다. 작은 기업이나 개인이 자원과 에너지의 엄청난 비용으로 거대한 모델을 처음부터 구축하고 훈련하는 것은 현실적이지 않습니다. 대신, 그들은 전이된 임베딩을 더 특화된 훈련 데이터를 사용하여 더 적은 양으로 미세 조정함으로써 특정 목표를 달성할 수 있습니다. DistilBERT 모델은 BERT의 기본 지식을 살펴본 훌륭한 예시로, 오픈 소스 커뮤니티(예: Hugging Face)에서 사용할 수 있습니다.

# 뇌에 대해 LLM이 우리에게 말해주는 것은?

## 그들은 같은 신경망 메커니즘을 공유하나요?

예기치 않게 정확하게 텍스트 작성, 번역 및 요약을 수행할 수 있는 LLM의 능력을 감안할 때, 트랜스포머가 단어와 문장을 처리할 때 인간 뇌처럼 작동하는지에 대한 중요한 질문이 제기됩니다.

<div class="content-ad"></div>

뇌의 뇌 영상 데이터를 사용한 간접적인 증거가 있어 뇌가 ~할지도 모릅니다. 그러나 연구 분야는 최근에 나타났고 결론은 여전히 애매합니다. 신경과학자인 Charlotte Caucheteux와 Jean-Rémi King은 Nature에 발표된 최근 논문에서 아주 잘 요약했습니다:

한편, 트랜스포머 아키텍처의 엄청난 성공은 신경과학자와 심리학자들에게 뇌의 인지 기능을 연구하는 접근 방식을 다시 고려하도록 도왔습니다. Goldstein 등은 다음과 같이 말했습니다:

인간 뇌에는 1000억 개 이상의 신경세포와 100조 개 이상의 시냅스가 있습니다. 대뇌 피질의 각 입방 밀리미터에는 약 5만 개의 신경세포가 있으며 이웃 및 먼 세포와 약 6000개의 조절 가능한 시냅스가 있습니다. 이것은 인간 뇌의 입방센티미터당 3억 개 이상의 조절 가능한 매개 변수를 갖게 됨을 의미합니다. 따라서 수백만에서 수십억 개의 매개 변수를 갖는 LLM은 학습할 수 있는 텍스트 및 지식의 양을 고려했을 때 비교할 만하다고 보입니다.

프린스턴 대학교 교수인 Uri Hasson의 발언을 보면, 트랜스포머에 의한 브루트 포스 학습은 과롭게 파라미터화된 간단한 모델로 진화 과정과 유사하여 "간단하고 간편한" 것으로 볼 수 있습니다. Hasson의 견해에 따르면 해석가능성의 부재는 결함이 아니라 파라미터 조정을 통해 순수하게 학습하는 결과물의 부산물일 수 있습니다. 진화가 결과물의 적합성을 기반으로 선택하지만 생물학적 설계나 특정 기능 수행 방식에 대해서는 신경 쓰지 않는 것처럼 말이죠. 최종 결과는 복잡한 세계에서의 궁극적인 유연성과 적응력입니다.

<div class="content-ad"></div>

## 맥락적 학습과 인덱싱된 메모리

Transformer의 주의는 인간의 주의와 똑같은가요? 우리는 뇌의 주의의 신경 기전에 대해 알려진 것이 매우 적기 때문에 알 수 없습니다.

인간의 주의는 현재 주요한 것을 학습하면서 뇌가 수신하는 대부분의 정보를 무시하기 위해 상태를 설정하며, 가장 관련이 많은 것을 강조합니다. 미국 심리학의 아버지인 윌리엄 제임스는 자신의 책 "심리학의 원리"에서 다음과 같이 썼습니다:

“주의는 마음이 여러 가지 동시에 발생할 수 있는 대상이나 생각 중 하나를 명확하고 생생하게 소유함을 의미합니다. 주의 집중, 의식의 집중이 그 본질입니다.”

<div class="content-ad"></div>

일반적으로 인간의 주의는 두 종류가 있어요: 명시적 주의와 암시적 주의입니다. 전자는 우리 눈으로 이끌어지며 시야의 초점(focal point)인 홍도(fovea: 시력이 가장 높은 망막 상의 영역)를 한 항목이나 위치에 다른 것들보다 더 집중하는 것을 의미합니다. 암시적 주의는 눈을 움직이지 않고도 정신적으로 주의를 옮기는 것으로, 전발엽 피질(prefrontal cortex)이 대뇌피질의 다른 영역들에 대해 실행적 통제 역할을 하는 것을 포함합니다.

트랜스포머(Transformer) 주의 메커니즘은 인간 주의와 유사한 효과를 달성합니다. 이는 단어 간 중요한 관계를 강화하고 모델에게 어떤 맥락과 단어에 "주의를 기울여야 하는지"를 알려줍니다. 따라서 트랜스포머 주의 메커니즘은 뇌의 기저에 대한 역학을 빛낼 수 있으며, 특히 어떻게 맥락 정보가 인코딩되고, 대체되고, 저장되며, 검색되는지에 대해 집중할 것입니다.

게다가, 트랜스포머의 자기 주의(self-attention)는 작업 기억(working memory)과 유사하며, 미래 효율적인 검색을 위해 주체와 관련된 맥락이 얽혀있어요. 인간 경험에서 우리는 단어를 더 잘 기억하는 경향이 있어요. 예를 들어, 예문, 이미지, 동의어 등과 함께 학습하면, 우리의 뇌에 추상적인 단어를 인코딩할 때 맥락과 유사한 단어들을 제공합니다. 이 인간의 기억 기술은 트랜스포머 주의 점수와 콘텐츠의 조정과도 닮아 있습니다. 신경세포들이 점곱(dot product)을 하는 생물학적 증거는 아직 없지만, 트랜스포머 주의 메커니즘은 뇌가 기능 수준에서 기억을 색인하는 방식과 일치하는 것으로 보입니다.

저의 이전 기사에서 논의했던 기억에 대한 맥락 의존적 게이팅(Context-Dependent Gating, XdG)은 ANN의 재앙적인 잊힘(catastrophic forgetting)을 극복하는 효과적인 방법입니다. 트랜스포머의 주의 메커니즘은 다른 학습된 가중치들을 교란시키지 않고 관련 파라미터를 선택적으로 강화하는 효과적인 방법을 제공해요. 학습의 LLM 전이 가능성은 트랜스포머 구조가 이전의 다른 ANN들이 겪었던 잊힘 문제를 경험하지 않음을 증명합니다. 이는 AI가 잊힘을 극복하고 지속적인 학습이 가능한 애플리케이션을 구축하기 위한 새로운 기회를 제공합니다.

<div class="content-ad"></div>

## LLM은 의식이 있을까요?

LLM의 놀라운 언어 능력은 그들이 자신이 하는 말에 대해 의식적이고 인식하고 있는지에 대한 중요한 질문을 던지게 했습니다. 이 질문은 사람들이 주로 언어를 사용하여 의식을 표현하고 소통하기 때문에 놀라운 것이 아닙니다.

의식에 대한 이전 기사에서 논의한 바와 같이 의식은 일반적으로 현재 경험, 깨어 있을 때 경험의 연속 및 핵심 자아 및 자서전적 자아(예: 사회적 자아, 영적 자아, 자부심)를 포함하는 세 가지 구성 요소를 갖고 있습니다. LLM에는 그중 어느 것도 없습니다.

인간의 경험은 항상 깨어 있는 동안에 현재에 존재하며 각 순간이 일관되고 독특합니다. 반면 LLM은 사람들의 질문이나 프롬프트에 반응하는 소프트웨어입니다. 이는 실시간으로 학습하거나 어떠한 자연 환경과 상호 작용하지 않습니다. 그것은 전문가 높은 차원의 임베딩 행렬을 사용하여 변압기 디코더 과정을 통해 가상으로 (사람들이 이용하는 물리적인 것과는 대조적으로) 반응하며 AI 엔지니어들에 의해 훈련되고 세밀하게 조정되고 품질이 테스트된 지식 시스템을 기반으로 합니다.

<div class="content-ad"></div>

그러나, 섭동(self-attention) 메커니즘 및 다중 모달 처리를 포함한 Transformer 아키텍처는 의식을 향한 슬라이더를 움직였습니다. 만약 LLM 모델이 센서와 모터와 연결되어 실시간으로 환경과 상호 작용한다면 어떨까요? LLM은 그럼 환경의 다양한 측면에 하나씩 주의를 기울여 지속적인 경험 사례를 생성할 수 있을까요?

재미있게도, 의식에 대한 많은 이론 중에서 Graziano와 Kastner는 주의 스키마 가설을 제안했습니다. 이 이론은 "인식은 주의 상태의 지각적 재구성"이라고 제안하며, 이 장치는 다른 사람의 인식 뿐만 아니라 우리 자신의 지각에 대한 인식도 지지합니다. 다시 말해, 현재 통합된 정보의 주의 기반 재구성은 셀프 인식의 의식을 불러일으키며 "뇌 속 사회적 장치"의 기초를 이룹니다.

더욱이, 이 이론은 "주의의 통제"의 뇌 책임과 상단에서 하향식의 대뇌 피로 경로에 의해 담당되는 "주의의 지각 표현"의 분리를 제안합니다. 이 이분법은 고차원 임베딩 표현에 대항하는 Transformer 섭동 메커니즘 프로세스와 관련이 있습니다. 이를 감안할 때, LLM은 의식에 관한 가설을 검증하는 데 사용될 수 있을 것입니다.

종합하면, 현재의 LLM이 무의식적인 시스템 1과 같다고 할지라도(노벨상 수상자 다니얼 카네먼의 책 "사고, 빠르고 느리게"에 언급된 내용), 유명 철학자와 AI 연구자들은 미래 세대의 LLM이 어떤 형태의 기계 의식과 시스템 2의 신중한 능력에 도달할 수 있을 것이라는 확신을 표명했습니다. 철학자 데이비드 찰머스가 최근 한 회의에서 말한 대로요:

<div class="content-ad"></div>

반면에, 변형기 구조와 LLM은 철학자, AI 연구자, 그리고 뇌과학자들에게 전례 없는 각도에서 의식을 연구할 수 있도록 영감을 줬어요. 의식을 연구하는 것은 어려웠던 것은, 그것이 사적이기 때문입니다 — 사람들은 자신의 의식을 사용해서만 그것을 불러내 볼 수 있어요. LLM은 의식의 정의를 보완하는 새로운 기회를 제공해 왔을 뿐만 아니라 미래에도 제공할 것입니다. 또한 뇌의 다양한 논란되는 의식 이론들에 대한 실험장이 될 수도 있어요.

# 결론

변형기 구조는 주목할만한 성과를 이룬 LLM의 획기적인 성과인 주의 메커니즘을 통해, AI 분야를 또 다른 중요한 이정표로 이끌어 냈어요. 컨볼루션 신경망(CNN)이 이미지 인식에서 인간 능력을 뛰어넘기 시작한 때부터 그리고 심층 강화 학습(RL)이 게임에서 세계 챔피언을 이기기 시작한 때와도 견줄 만합니다.

지난 몇 십 년을 돌아보면, 성공적인 AI 모델은 모두 동일한 특징을 나타내요: 많은 매개변수 최적화, 거대한 양의 훈련 데이터, 방대한 컴퓨팅 파워, 그리고 최소한의 명시적인 규칙이나 복잡한 알고리즘입니다. LLM은 다양한 다중 모달리티와 인상적인 확장성을 표현함으로써 다른 정점에 다다랐어요.

<div class="content-ad"></div>

CNN과 딥 강화 학습과는 달리, 트랜스포머의 수학적 아름다움은 아직 생물학적 두뇌에서 찾아지지 않았습니다. 모든 AI 결과가 두뇌를 모방할 필요는 없지만, 트랜스포머는 신경과학자들에게 인간 정신의 내부 구조를 연구하는 새로운 이론과 영감을 제공해 왔습니다.